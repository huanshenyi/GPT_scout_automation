{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4NNeIJACEity+jx2Oz/lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huanshenyi/GPT_scout_automation/blob/main/GPT_model_fine_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルのファインチューニング\n",
        "\n",
        "ファインチューニングにより、モデルから漱石風の文章が生成されるようになることを確認しましょう。\n",
        "\n",
        "学習に時間がかかるので、「編集」→「ノートブックの設定」の「ハードウェアアクセラレーター」で「GPU」を選択しましょう。"
      ],
      "metadata": {
        "id": "nPJyS8sP0meM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ライブラリのインストール\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "wpO5ww8M6J9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ベースモデルのロード\n",
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")"
      ],
      "metadata": {
        "id": "K16ArZmg-JQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 学習用ファイルの読み込み\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # ファイルのアップロード\n",
        "file_path_original = list(uploaded.keys())[0]  # ファイルパス\n",
        "print(file_path_original)  "
      ],
      "metadata": {
        "id": "Q0hVJqbT_f9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## アップロードされたファイルを読み込み、一部を表示します。\n",
        "with open(file_path_original, mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    text_original = f.read()\n",
        "\n",
        "print(text_original[:100])  # 最初の100文字を表示"
      ],
      "metadata": {
        "id": "ZXhZ6FZyArhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 正規表現を使い、前処理を行う\n",
        "import re\n",
        "\n",
        "text = re.sub(\"《[^》]+》\", \"\", text_original)  # ルビの削除\n",
        "text = re.sub(\"［[^］]+］\", \"\", text)  # 読みの注意の削除\n",
        "text = re.sub(\"[｜ 　]\", \"\", text)  # | と全角半角スペースの削除\n",
        "print(text[:100])  # 最初の100文字を表示\n",
        "\n",
        "train_data_path = \"train.txt\"\n",
        "with open(train_data_path, mode=\"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "_Xnr0SPcCRIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの訓練\n",
        "\n",
        "既存のモデルに追加で訓練を行います。\n",
        "\n",
        "まずは、訓練のための各設定を行います。"
      ],
      "metadata": {
        "id": "UMxzDjp_JF2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "\n",
        "# データセットの設定\n",
        "train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_data_path,\n",
        "        block_size=128  # 文章の長さ\n",
        "        )\n",
        "\n",
        "# データの入力に関する設定\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # データをマスクするかどうか\n",
        "    )\n",
        "\n",
        "# 訓練に関する設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-ft\",  # 関連ファイルを保存するパス\n",
        "    overwrite_output_dir=True,  # ファイルを上書きするかどうか\n",
        "    num_train_epochs=3,  # エポック数\n",
        "    per_device_train_batch_size=8,  # バッチサイズ\n",
        "    logging_steps=100,  # 途中経過を表示する間隔\n",
        "    save_steps=800  # モデルを保存する間隔\n",
        "    )\n",
        "\n",
        "# トレーナーの設定\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "4LAEPRmMJJ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーナーのtrain()メソッドにより、訓練が開始されます。\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "gEOQ5MvFJ6ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文章を生成する関数\n",
        "# 入力文章から続きの文章を生成する関数を設定します。\n",
        "def getarate_sentences(seed_sentence):\n",
        "    x = tokenizer.encode(seed_sentence, return_tensors=\"pt\", add_special_tokens=False)  # 入力\n",
        "    x = x.cuda()  # GPU対応\n",
        "    y = model.generate(x, #　入力\n",
        "                       min_length=50,  # 文章の最小長\n",
        "                       max_length=100,  # 文章の最大長\n",
        "                       do_sample=True,   # 次の単語を確率で選ぶ\n",
        "                       top_k=50, # Top-Kサンプリング\n",
        "                       top_p=0.95,  # Top-pサンプリング\n",
        "                       temperature=1.2,  # 確率分布の調整\n",
        "                       num_return_sequences=3,  # 生成する文章の数\n",
        "                       pad_token_id=tokenizer.pad_token_id,  # パディングのトークンID\n",
        "                       bos_token_id=tokenizer.bos_token_id,  # テキスト先頭のトークンID\n",
        "                       eos_token_id=tokenizer.eos_token_id,  # テキスト終端のトークンID\n",
        "                    #    bad_word_ids=[[tokenizer.unk_token_id]]  # 生成が許可されないトークンID\n",
        "                       )  \n",
        "    generated_sentences = tokenizer.batch_decode(y, skip_special_tokens=True)  # 特殊トークンをスキップして文章に変換\n",
        "    return generated_sentences"
      ],
      "metadata": {
        "id": "tr16OS-aQiLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章の生成\n",
        "\n",
        "「吾輩は猫である」の冒頭をシードにして、ファインチューニング済みのGPT-2モデルにより小説を執筆します。"
      ],
      "metadata": {
        "id": "JH0E7nyGQosg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_sentence = \"吾輩は猫である。名前はまだ無い。\"  # 吾輩は猫であるの冒頭\n",
        "generated_sentences = getarate_sentences(seed_sentence)  # 生成された文章\n",
        "for sentence in generated_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "pxdDWgahQqSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}